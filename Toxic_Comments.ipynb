{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Toxic Comments",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1t83cNrhc4B8-Q-mokjAM2tO4iQsW8iVl",
      "authorship_tag": "ABX9TyOINe/lnvQ0Ijhzo8EsX84x"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6b9IvDtLWuJ",
        "colab_type": "text"
      },
      "source": [
        "# Data Prep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAcgqTXfLgR2",
        "colab_type": "text"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0bxExDwLj6a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "c4a4fa11-a47c-4753-d95e-8d7255b7d72f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "dwlr = nltk.downloader.Downloader()\n",
        "for pkg in dwlr.packages():\n",
        "  if pkg.subdir == 'tokenizers':\n",
        "    dwlr.download(pkg.id)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9M8O2GfLe0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import joblib\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97eaoeX6RZWB",
        "colab_type": "text"
      },
      "source": [
        "## Initialize Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9_ynE5GK4VM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_DIR = '/content/drive/My Drive/Machine Learning/Data/Toxic comments/'\n",
        "EMBEDDING_DIR = '/content/drive/My Drive/Machine Learning/Data/pre-trained embeddings/'\n",
        "TRAIN_DATA = 'toxic_train.csv'\n",
        "TEST_DATA = 'toxic_test.csv'\n",
        "GLOVE_FILE = 'glove.6B.300d.txt'\n",
        "TOKENIZER_FILE = 'toxic_tokenizer.pkl'\n",
        "WORD_EMBEDDING_MATRIX = 'word_embedding_matrix.npy'\n",
        "NB_WORDS_FILE = 'nb_words.json'\n",
        "PROCESSED_TRAIN_DATA = 'train.npy'\n",
        "PROCESSED_TEST_DATA = 'test.npy'\n",
        "TARGET_DATA = 'target.npy'\n",
        "MAX_NB_WORDS = 200000\n",
        "MAX_SEQUENCE_LENGTH = 150\n",
        "EMBEDDING_DIM = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAINDA3oRpln",
        "colab_type": "text"
      },
      "source": [
        "## Download and clean data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDBh3eQXLNTP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7c526f8d-bff2-4cc4-c838-3c163256d0cd"
      },
      "source": [
        "# read data\n",
        "train = pd.read_csv(BASE_DIR + TRAIN_DATA)\n",
        "\n",
        "# check if there is any missing values\n",
        "missing_values = train['comment_text'].isna().sum()\n",
        "\n",
        "comments = train['comment_text']\n",
        "labels = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
        "y = train[labels].values\n",
        "\n",
        "print(f\"# of comments: {len(comments)}\")\n",
        "print(f\"# of missing values: {missing_values}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of comments: 159571\n",
            "# of missing values: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWX-F4ACSCEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# functions to clean comments\n",
        "def clean_comments(comment):\n",
        "  tokens = word_tokenize(comment)\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [w for w in tokens if not w in stop_words]\n",
        "  tokens = [w for w in tokens if w.isalpha()]\n",
        "  tokens = ' '.join(tokens)\n",
        "  return tokens\n",
        "\n",
        "\n",
        "def process_comments(comment):\n",
        "  clean_c = []\n",
        "  for c in comment:\n",
        "    c = str(c)\n",
        "    c = clean_comments(c)\n",
        "    clean_c.append(c)\n",
        "  return clean_c\n",
        "\n",
        "comments_clean = process_comments(comments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UalhaQ8GVDKN",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize words and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B__ZdYbqVF1b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "22aa463b-8880-4317-c16d-3f040eb61888"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(comments_clean)\n",
        "\n",
        "comments_word_sequences = tokenizer.texts_to_sequences(comments_clean)\n",
        "comments_data = pad_sequences(comments_word_sequences, maxlen = MAX_SEQUENCE_LENGTH, padding = 'post')\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f\"# of words in index: {len(word_index)}\")\n",
        "print(f\"max length of a sequence: {max([len(s.split()) for s in comments_clean])}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of words in index: 157448\n",
            "max length of a sequence: 1383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx4IFG_cWUjh",
        "colab_type": "text"
      },
      "source": [
        "## Process embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4zzcy_tWE7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b33f164f-324c-4239-ced6-c566a317c79e"
      },
      "source": [
        "# define functions to process embedding\n",
        "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
        "\n",
        "def load_embedding(filepath):\n",
        "  file = open(filepath, 'r', encoding = 'utf-8')\n",
        "  embeddings = {}\n",
        "  for line in file:\n",
        "    values = line.split(' ')\n",
        "    word = values[0]\n",
        "    vec = np.asarray(values[1:], dtype = 'float32')\n",
        "    embeddings[word] = vec\n",
        "  return embeddings\n",
        "\n",
        "\n",
        "def get_weight_matrix(embedding):\n",
        "  weight_matrix = np.zeros((nb_words+1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "    if i >= MAX_NB_WORDS:\n",
        "      continue\n",
        "    embedding_vector = embedding.get(word)\n",
        "    if embedding_vector is not None: # there may be words in our data that's missing from embedding\n",
        "      weight_matrix[i] = embedding_vector\n",
        "  return weight_matrix\n",
        "\n",
        "raw_embedding = load_embedding(EMBEDDING_DIR + GLOVE_FILE)\n",
        "word_embedding_matrix = get_weight_matrix(raw_embedding)\n",
        "\n",
        "print(f\"embedding shape: {word_embedding_matrix.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding shape: (157449, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT1CGrA2ULTg",
        "colab_type": "text"
      },
      "source": [
        "## Process test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5H0Kq0pUK4q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "56a1d7a8-6c46-4917-848e-daa6742b8d53"
      },
      "source": [
        "# test data\n",
        "test = pd.read_csv(BASE_DIR + TEST_DATA)\n",
        "\n",
        "# check missing values\n",
        "test_missing_values = test['comment_text'].isnull().sum()\n",
        "\n",
        "test_comments = test['comment_text']\n",
        "\n",
        "print(f\"# of test comments: {len(test_comments)}\")\n",
        "print(f\"# missing values: {test_missing_values}\")\n",
        "\n",
        "# clean comments\n",
        "test_clean = process_comments(test_comments)\n",
        "# tokenize test data\n",
        "test_word_sequences = tokenizer.texts_to_sequences(test_clean)\n",
        "test_data = pad_sequences(test_word_sequences, maxlen = MAX_SEQUENCE_LENGTH, padding = 'post')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# of test comments: 153164\n",
            "# missing vallues: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrI2GTQkYvt7",
        "colab_type": "text"
      },
      "source": [
        "## Save files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bdxy2OEXQuL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.save(open(BASE_DIR + PROCESSED_TRAIN_DATA, 'wb'), comments_data)\n",
        "np.save(open(BASE_DIR + WORD_EMBEDDING_MATRIX, 'wb'), word_embedding_matrix)\n",
        "np.save(open(BASE_DIR + TARGET_DATA, 'wb'), y)\n",
        "np.save(open(BASE_DIR + PROCESSED_TEST_DATA, 'wb'), test_data)\n",
        "joblib.dump(tokenizer, BASE_DIR + TOKENIZER_FILE)\n",
        "with open(BASE_DIR + NB_WORDS_FILE, 'w') as f:\n",
        "  json.dump({'nb_words':nb_words}, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sltHaMz4edr_",
        "colab_type": "text"
      },
      "source": [
        "# Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOeSSWmbgA2o",
        "colab_type": "text"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71yLcIdsf-57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, Dropout, BatchNormalization, LSTM, Bidirectional\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qz2p4AMgod7",
        "colab_type": "text"
      },
      "source": [
        "## Initialize variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBkhv-Phb6JB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_PATH = 'lstm_toxic_comment_model.h5'\n",
        "MAX_SEQUENCE_LENGTH = 150\n",
        "EMBEDDING_DIM = 300\n",
        "VALIDATION_SPLIT = 0.2\n",
        "EPOCHS = 15\n",
        "DROPOUT = 0.2\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_w88cpahAAs",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNKRksRshBbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "comments = np.load(open(BASE_DIR + PROCESSED_TRAIN_DATA, 'rb'))\n",
        "target = np.load(open(BASE_DIR + TARGET_DATA, 'rb'))\n",
        "test = np.load(open(BASE_DIR + PROCESSED_TEST_DATA, 'rb'))\n",
        "word_embedding_matrix = np.load(open(BASE_DIR + WORD_EMBEDDING_MATRIX, 'rb'))\n",
        "with open(BASE_DIR + NB_WORDS_FILE, 'r') as f:\n",
        "  nb_words = json.load(f)['nb_words']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF5oSKlQi4XU",
        "colab_type": "text"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dfMZDHmh70a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "9270017a-95dc-4160-c8f4-401eb6ac09fa"
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "input = Input(shape = (MAX_SEQUENCE_LENGTH,))\n",
        "\n",
        "x = Embedding(nb_words+1,\n",
        "              EMBEDDING_DIM,\n",
        "              input_length = MAX_SEQUENCE_LENGTH,\n",
        "              weights = [word_embedding_matrix],\n",
        "              trainable = False)(input)\n",
        "x = Bidirectional(LSTM(128, return_sequences = False))(x)\n",
        "x = Dense(16,activation = 'relu')(x)\n",
        "x = Dropout(DROPOUT)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(6, activation = 'sigmoid')(x)\n",
        "\n",
        "model = Model(inputs = input, outputs = x)\n",
        "opt = Adam(lr = 0.001)\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 150)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 150, 300)          47234700  \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 256)               439296    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 16)                4112      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 102       \n",
            "=================================================================\n",
            "Total params: 47,678,274\n",
            "Trainable params: 443,542\n",
            "Non-trainable params: 47,234,732\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QunkohXEkGu6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "2bbe3752-3155-4dd7-84ed-5f025388d608"
      },
      "source": [
        "t0 = time.time()\n",
        "es = EarlyStopping(monitor = 'val_loss', patience = 3, mode = 'auto', verbose=0)\n",
        "cp = ModelCheckpoint(BASE_DIR + MODEL_PATH, monitor = 'val_loss', verbose = 0, save_best_only = True)\n",
        "\n",
        "history = model.fit(comments,\n",
        "                    target, \n",
        "                    verbose = 2,\n",
        "                    batch_size = BATCH_SIZE,\n",
        "                    epochs = EPOCHS,\n",
        "                    callbacks = [es, cp],\n",
        "                    validation_split = VALIDATION_SPLIT)\n",
        "t1 = time.time()\n",
        "\n",
        "print(f\"Total training time elapsed: {np.round(((t1-t0)/60),2)} minutes\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "3990/3990 - 73s - loss: 0.1081 - accuracy: 0.9140 - val_loss: 0.0565 - val_accuracy: 0.9940\n",
            "Epoch 2/15\n",
            "3990/3990 - 70s - loss: 0.0648 - accuracy: 0.9941 - val_loss: 0.0566 - val_accuracy: 0.9941\n",
            "Epoch 3/15\n",
            "3990/3990 - 72s - loss: 0.0590 - accuracy: 0.9939 - val_loss: 0.0537 - val_accuracy: 0.9940\n",
            "Epoch 4/15\n",
            "3990/3990 - 70s - loss: 0.0557 - accuracy: 0.9921 - val_loss: 0.0540 - val_accuracy: 0.9935\n",
            "Epoch 5/15\n",
            "3990/3990 - 70s - loss: 0.0529 - accuracy: 0.9815 - val_loss: 0.0540 - val_accuracy: 0.9923\n",
            "Epoch 6/15\n",
            "3990/3990 - 70s - loss: 0.0504 - accuracy: 0.9375 - val_loss: 0.0541 - val_accuracy: 0.9757\n",
            "Total training time elapsed: 7.11 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}